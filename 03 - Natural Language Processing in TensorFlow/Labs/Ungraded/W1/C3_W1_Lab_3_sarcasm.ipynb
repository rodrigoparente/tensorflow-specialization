{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdNGfEo2u-r7"
   },
   "source": [
    "# Ungraded Lab: Tokenizing the Sarcasm Dataset\n",
    "\n",
    "In this lab, you will apply what you've learned in the past two exercises to preprocess the [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection). This contains news headlines which are labeled as sarcastic or not. You will revisit this dataset in later labs so it is good to be acquainted with it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1c0PdMndNKY"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Let's start by importing the packages and methods you will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m19wi0ehcoqh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 09:59:35.546410: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-07 09:59:35.566822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-07 09:59:35.611658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-07 09:59:35.625314: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-07 09:59:35.655128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-07 09:59:36.914648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twhyfjg0xTkg"
   },
   "source": [
    "## Download and inspect the dataset\n",
    "\n",
    "Then, you will fetch the dataset and preview some of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "33W129a7xgoJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-07 09:59:38--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.29.91, 172.217.30.27, 142.250.219.251, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.29.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: ‘data/sarcasm.json’\n",
      "\n",
      "sarcasm.json        100%[===================>]   5.38M  10.5MB/s    in 0.5s    \n",
      "\n",
      "2024-10-07 09:59:39 (10.5 MB/s) - ‘data/sarcasm.json’ saved [5643545/5643545]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget -P data/ -nc https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJHdzh9FyWa2"
   },
   "source": [
    "The dataset is saved as a [JSON](https://www.json.org/json-en.html) file and you can use Python's [`json`](https://docs.python.org/3/library/json.html) module to load it into your workspace. The cell below unpacks the JSON file into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OkaBMeNDwMel"
   },
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(\"./data/sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2aSBvJVzRNV"
   },
   "source": [
    "You can inspect a few of the elements in the list. You will notice that each element consists of a dictionary with a URL link, the actual headline, and a label named `is_sarcastic`. Printed below are two elements with contrasting labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RiiFcWU2xnMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n",
      "{'article_link': 'https://www.theonion.com/pediatricians-announce-2011-newborns-are-ugliest-babies-1819572977', 'headline': 'pediatricians announce 2011 newborns are ugliest babies in 30 years', 'is_sarcastic': 1}\n"
     ]
    }
   ],
   "source": [
    "# Non-sarcastic headline\n",
    "print(datastore[0])\n",
    "\n",
    "# Sarcastic headline\n",
    "print(datastore[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPuH0bBiz8LJ"
   },
   "source": [
    "With that, you can collect the headlines because those are the string inputs that you will preprocess into numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9pxLUQJCxkNB"
   },
   "outputs": [],
   "source": [
    "# Append the headline elements into the list\n",
    "sentences = [item['headline'] for item in datastore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBHSXJ5V0qqK"
   },
   "source": [
    "## Preprocessing the headlines\n",
    "\n",
    "You can convert the sentences list above into padded sequences by using the same methods you've been using in the previous labs. The cells below will build the vocabulary, then use that to generate the list of post-padded sequences for each of the 26,709 headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "enHwI8WwLyyl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728305979.359494   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.432992   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.433037   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.440408   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.440484   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.440498   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.770054   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728305979.770128   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-07 09:59:39.770142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728305979.770182   50446 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-07 09:59:39.770207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Apply the layer for post padding\n",
    "post_padded_sequences = vectorize_layer(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VbRue1ySFzo"
   },
   "source": [
    "You can view the results for a particular headline by changing the value of `index` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9p_iwAbrJV_Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "padded sequence: [  140   825     2   813  1100  2048   571  5057   199   139    39    46\n",
      "     2 13050     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "\n",
      "shape of padded sequences: (26709, 39)\n"
     ]
    }
   ],
   "source": [
    "# Print a sample headline and sequence\n",
    "index = 2\n",
    "print(f'sample headline: {sentences[index]}')\n",
    "print(f'padded sequence: {post_padded_sequences[index]}')\n",
    "print()\n",
    "\n",
    "# Print dimensions of padded sequences\n",
    "print(f'shape of padded sequences: {post_padded_sequences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cguoGA9veJLN"
   },
   "source": [
    "For prepadding, you have to setup the `TextVectorization` layer differently. You don't want to have the automatic postpadding shown above, and instead have sequences with variable length. Then, you will pass it to the `pad_sequences()` utility function you used in the previous lab. The cells below show one way to do it:\n",
    "\n",
    "* First, you will initialize the `TextVectorization` layer and set its `ragged` flag to `True`. This will result in a [ragged tensor](https://www.tensorflow.org/guide/ragged_tensor) which simply means a tensor with variable-length elements. The sequences will indeed have different lengths after removing the zeroes, thus you will need the ragged tensor to contain them.\n",
    "\n",
    "* Like before, you will use the layer's `adapt()` method to generate a vocabulary.\n",
    "\n",
    "* Then, you will apply the layer to the string sentences to generate the integer sequences. As mentioned, this will not be post-padded.\n",
    "\n",
    "* Lastly, you will pass this ragged tensor to the [pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function to generate pre-padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QxHarVDjws5b"
   },
   "outputs": [],
   "source": [
    "# Instantiate the layer and set the `ragged` flag to `True`\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "\n",
    "# Build the vocabulary\n",
    "vectorize_layer.adapt(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tNlmYjgUx-qk"
   },
   "outputs": [],
   "source": [
    "# Apply the layer to generate a ragged tensor\n",
    "ragged_sequences = vectorize_layer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vZnXkNILyAVb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "padded sequence: [  140   825     2   813  1100  2048   571  5057   199   139    39    46\n",
      "     2 13050]\n",
      "\n",
      "shape of padded sequences: (26709, None)\n"
     ]
    }
   ],
   "source": [
    "# Print a sample headline and sequence\n",
    "index = 2\n",
    "print(f'sample headline: {sentences[index]}')\n",
    "print(f'padded sequence: {ragged_sequences[index]}')\n",
    "print()\n",
    "\n",
    "# Print dimensions of padded sequences\n",
    "print(f'shape of padded sequences: {ragged_sequences.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TNkugcfkPv1w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,   140,   825,\n",
       "           2,   813,  1100,  2048,   571,  5057,   199,   139,    39,\n",
       "          46,     2, 13050], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply pre-padding to the ragged tensor\n",
    "pre_padded_sequences = pad_sequences(ragged_sequences.numpy())\n",
    "\n",
    "# Preview the result for the 2nd sequence\n",
    "pre_padded_sequences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izkWrVNKQwJQ"
   },
   "source": [
    "You can see the results for post-padded and pre-padded sequences by changing the value of `index` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MtgG2CMtPbN2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample headline: mom starting to fear son's web series closest thing she will have to grandchild\n",
      "\n",
      "post-padded sequence: [  140   825     2   813  1100  2048   571  5057   199   139    39    46\n",
      "     2 13050     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "\n",
      "pre-padded sequence: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   140   825     2   813  1100  2048   571  5057   199   139    39\n",
      "    46     2 13050]\n",
      "\n",
      "shape of post-padded sequences: (26709, 39)\n",
      "shape of pre-padded sequences: (26709, 39)\n"
     ]
    }
   ],
   "source": [
    "# Print a sample headline and sequence\n",
    "index = 2\n",
    "print(f'sample headline: {sentences[index]}')\n",
    "print()\n",
    "print(f'post-padded sequence: {post_padded_sequences[index]}')\n",
    "print()\n",
    "print(f'pre-padded sequence: {pre_padded_sequences[index]}')\n",
    "print()\n",
    "\n",
    "# Print dimensions of padded sequences\n",
    "print(f'shape of post-padded sequences: {post_padded_sequences.shape}')\n",
    "print(f'shape of pre-padded sequences: {pre_padded_sequences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wyLF5T036W8"
   },
   "source": [
    "This concludes the short demo on text data preprocessing on a relatively large dataset. Next week, you will start building models that can be trained on these output sequences. See you there!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow-specialization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
