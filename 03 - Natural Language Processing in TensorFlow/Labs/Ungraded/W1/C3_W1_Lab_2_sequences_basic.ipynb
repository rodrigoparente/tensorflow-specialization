{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SmE2CODfmmL"
   },
   "source": [
    "# Ungraded Lab: Generating Sequences and Padding\n",
    "\n",
    "In this lab, you will look at converting input sentences into numeric sequences. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiFUJg-lmTm6"
   },
   "source": [
    "## Text to Sequences\n",
    "\n",
    "In the previous lab, you saw how to use the `TextVectorization` layer to build a vocabulary from your corpus. It generates a list where more frequent words have lower indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LXzsIYWMvFM-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 13:51:20.125430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-06 13:51:18.143110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-06 13:51:18.281446: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-06 13:51:18.311443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-06 13:51:18.508968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 13:51:19.814994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728233482.692393    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233482.893728    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233482.893777    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233482.898022    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233482.898066    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233482.898087    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233483.498155    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728233483.498217    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 13:51:23.498230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728233483.498265    8944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 13:51:23.498908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 my\n",
      "3 love\n",
      "4 dog\n",
      "5 you\n",
      "6 i\n",
      "7 think\n",
      "8 is\n",
      "9 do\n",
      "10 cat\n",
      "11 amazing\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sample inputs\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "    ]\n",
    "\n",
    "# Initialize the layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Compute the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Print the token index\n",
    "for index, word in enumerate(vocabulary):\n",
    "  print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VNFxYidr9qg"
   },
   "source": [
    "You can then use the result to convert each of the input sentences into integer sequences. See how that's done below given a single input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lQWcXlE1saUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([6 3 2 4], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# String input\n",
    "sample_input = 'I love my dog'\n",
    "\n",
    "# Convert the string input to an integer sequence\n",
    "sequence = vectorize_layer(sample_input)\n",
    "\n",
    "# Print the result\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZZnENfZtoiA"
   },
   "source": [
    "As shown, you simply pass in the string to the layer which already learned the vocabulary, and it will output the integer sequence as a `tf.Tensor`. In this case, the result is `[6 3 2 4]`. You can look at the token index printed above to verify that it matches the indices for each word in the input string.\n",
    "\n",
    "For a given list of string inputs (such as the 4-item `sentences` list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the `map()` method and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "41foIDBQw3FA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love my dog ---> [6 3 2 4]\n",
      "I love my cat ---> [ 6  3  2 10]\n",
      "You love my dog! ---> [5 3 2 4]\n",
      "Do you think my dog is amazing? ---> [ 9  5  7  2  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list to tf.data.Dataset\n",
    "sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# Define a mapping function to convert each sample input\n",
    "sequences = sentences_dataset.map(vectorize_layer)\n",
    "\n",
    "# Print the integer sequences\n",
    "for sentence, sequence in zip(sentences, sequences):\n",
    "  print(f'{sentence} ---> {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV_91IQB62R0"
   },
   "source": [
    "As you can see, each sentence is successfully transformed into an integer sequence. The problem with this is they have varying lengths so it cannot be consumed by the model right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z56pEkF2p8c-"
   },
   "source": [
    "## Padding\n",
    "\n",
    "You can get a list of varying lengths to have a uniform size by padding or truncating tokens from the sequences. Padding is more common to preserve information.\n",
    "\n",
    "Recall that your vocabulary reserves a special token index `0` for padding. It will add that token (called post padding) if you pass in a list of string inputs to the layer. See an example below. Notice that you have the same output as above but the integer sequences are already post-padded with `0` up to the length of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DJpjZvG9wtLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n",
      "\n",
      "OUTPUT:\n",
      "tf.Tensor(\n",
      "[[ 6  3  2  4  0  0  0]\n",
      " [ 6  3  2 10  0  0  0]\n",
      " [ 5  3  2  4  0  0  0]\n",
      " [ 9  5  7  2  4  8 11]], shape=(4, 7), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Apply the layer to the string input list\n",
    "sequences_post = vectorize_layer(sentences)\n",
    "\n",
    "# Print the results\n",
    "print('INPUT:')\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(sequences_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHqYAVmNAi5D"
   },
   "source": [
    "If you want pre-padding, you can use the [pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) utility to prepend a padding token to the sequences. Notice that the `padding` argument is set to `pre`. This is just for clarity. The function already has this set as the default so you can opt to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qljgx1eSlEse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "[6 3 2 4]\n",
      "[ 6  3  2 10]\n",
      "[5 3 2 4]\n",
      "[ 9  5  7  2  4  8 11]\n",
      "\n",
      "OUTPUT:\n",
      "[[ 0  0  0  6  3  2  4]\n",
      " [ 0  0  0  6  3  2 10]\n",
      " [ 0  0  0  5  3  2  4]\n",
      " [ 9  5  7  2  4  8 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 13:53:30.686446: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-10-06 13:53:30.710758: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Pre-pad the sequences to a uniform length.\n",
    "# You can remove the `padding` argument and get the same result.\n",
    "sequences_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "# Print the results\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(sequences_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsgHsYP2DDrQ"
   },
   "source": [
    "If you switch the `padding` argument to `post`, you will arrive at the same result as applying the layer directly.\n",
    "\n",
    "The function also has a `maxlen` argument that you can use to truncate tokens from the sequences. By default, it will drop tokens in front. If you want to drop tokens at the other end, you will have to set the `truncating` argument to `post`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "70dmSqBDAbYH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "[6 3 2 4]\n",
      "[ 6  3  2 10]\n",
      "[5 3 2 4]\n",
      "[ 9  5  7  2  4  8 11]\n",
      "\n",
      "OUTPUT:\n",
      "[[ 0  6  3  2  4]\n",
      " [ 0  6  3  2 10]\n",
      " [ 0  5  3  2  4]\n",
      " [ 7  2  4  8 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 13:53:49.383196: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Post-pad the sequences and limit the size to 5.\n",
    "sequences_post_trunc = tf.keras.utils.pad_sequences(sequences, maxlen=5, padding='pre')\n",
    "\n",
    "# Print the results\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(sequences_post_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to prepare your sequences for prepadding is to set the TextVectorization to output a ragged tensor. This means the output will not be automatically post-padded. See the output sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[6, 3, 2, 4], [6, 3, 2, 10], [5, 3, 2, 4], [9, 5, 7, 2, 4, 8, 11]]>\n"
     ]
    }
   ],
   "source": [
    "# Set the layer to output a ragged tensor\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "\n",
    "# Compute the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Apply the layer to the sentences\n",
    "ragged_sequences = vectorize_layer(sentences)\n",
    "\n",
    "# Print the results\n",
    "print(ragged_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, you can now pass it directly to the `pad_sequences()` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  6  3  2  4]\n",
      " [ 0  0  0  6  3  2 10]\n",
      " [ 0  0  0  5  3  2  4]\n",
      " [ 9  5  7  2  4  8 11]]\n"
     ]
    }
   ],
   "source": [
    "# Pre-pad the sequences in the ragged tensor\n",
    "sequences_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n",
    "\n",
    "# Print the results\n",
    "print(sequences_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btEb9jI0k7Ip"
   },
   "source": [
    "## Out-of-vocabulary tokens\n",
    "\n",
    "Lastly, you'll see what the other special token is for. The layer will use the token index `1` when you have input words that are not found in the vocabulary list. For example, you may decide to collect more text after your initial training and decide to not recompute the vocabulary. You will see this in action in the cell below. Notice that the token `1` is inserted for words that are not found in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4fW1NWTok72V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really love my dog ---> [6 1 3 2 4]\n",
      "my dog loves my manatee ---> [2 4 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Try with words that are not in the vocabulary\n",
    "sentences_with_oov = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "# Generate the sequences\n",
    "sequences_with_oov = vectorize_layer(sentences_with_oov)\n",
    "\n",
    "# Print the integer sequences\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "  print(f'{sentence} ---> {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBlQIPBqskAJ"
   },
   "source": [
    "This concludes another introduction to text data preprocessing. So far, you've just been using dummy data. In the next exercise, you will be applying the same concepts to a real-world and much larger dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow-specialization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
